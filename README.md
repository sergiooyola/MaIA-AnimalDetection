1. Convertir las anotaciones de csv a json: EDA_Validacion_Export_COCO_Dataset.ipynb
2. Pasar las imagenes a tiles: Genera_tiles.ipynb
3. Entrenar, Probar resultados y estadisticas: Train_Faster_R_CNN_sobre_tiles.ipynb

<strong>Detalle EDA_Validacion_Export_COCO_Dataset.ipynb</strong>
   1) Instalaci√≥n y carga de librer√≠as
Ejecuta pip install para tener pandas, numpy, matplotlib, seaborn, opencv-python, pycocotools, tqdm.
Importa los paquetes y configura seaborn con estilo ‚Äúwhitegrid‚Äù.
Mensaje de control: ‚úÖ Librer√≠as cargadas.

2) Par√°metros de entrada (rutas)
Define el tipo de dato (DATA_TYPE = 'csv') y rutas a:
CSV de train, val, test (con columnas: Image, x1, y1, x2, y2, Label).
Carpetas de im√°genes (train/val/test).
Carpeta OUTPUT_DIR donde guardar√° los JSON convertidos a COCO.
Aqu√≠ decides desde d√≥nde leer y d√≥nde escribir.

3) Utilidades: carga y conversi√≥n a COCO
load_csv(path)
Lee el CSV con pandas.
Limpia nombres de columnas (quita espacios).
Muestra cu√°ntos registros trae: üìÑ CSV con N registros.
csv_to_coco(csv_path, images_dir, output_json)
Convierte tu CSV a un JSON en formato COCO:

Construye:
categories: a partir de los Label √∫nicos en el CSV (los mapea a ids 1..K).
images: una entrada por imagen (con id incremental, file_name, width, height).
La funci√≥n, por la estructura t√≠pica, infiere ancho/alto abriendo la imagen o los puede tomar si el CSV lo ofrece (seg√∫n la versi√≥n de tu cuaderno; en el tuyo construye la lista bas√°ndose en los registros y asocia cada nombre de imagen con un image_id).
annotations: una entrada por bounding box (con:
image_id, category_id,
bbox en COCO [x, y, w, h] (lo calcula a partir de tus columnas x1,y1,x2,y2),
area = w*h,
iscrowd = 0).
Guarda el JSON final con:
{
  "info": {...},
  "licenses": [],
  "images": [...],
  "annotations": [...],
  "categories": [...]
}
Mensaje de √©xito: ‚úÖ Guardado <ruta_del_json>.
Resultado: tendr√°s, por ejemplo, train_converted.json (y m√°s adelante tambi√©n los de val y test).

4) EDA de cajas (an√°lisis exploratorio)
analyze_boxes(df)
Calcula anchos y altos a partir de x2-x1 y y2-y1.
Muestra un describe() con count/mean/std/min/percentiles/max para width y height.
Grafica histogramas de distribuci√≥n de anchos y altos.
suggest_anchors(stats)
Toma la media de width y height y sugiere una rejilla t√≠pica de anchor_sizes: [8,16,32,64,128].
Imprime el tama√±o promedio y la lista sugerida.
Con esto te haces una idea del tama√±o t√≠pico de objeto y si tus anchors default son razonables.

5) EDA r√°pida sobre train y conversi√≥n a COCO (train)
df_train = load_csv(TRAIN_PATH)
value_counts() de Label y gr√°fica de barras con la distribuci√≥n de clases.
stats = analyze_boxes(df_train) y suggest_anchors(stats) para ver tama√±os.
Convierte el CSV de train a COCO:
csv_to_coco(TRAIN_PATH, TRAIN_IMG_DIR, os.path.join(OUTPUT_DIR,'train_converted.json'))

6) Validaci√≥n de estructura COCO (train)
Carga el JSON convertido con pycocotools:
coco = COCO(os.path.join(OUTPUT_DIR, 'train_converted.json'))

Imprime:
üì∑ Im√°genes: <n_imgs>
üì¶ Anotaciones: <n_anns>
üêæ Categor√≠as: <n_cats> ‚Üí [primeras categor√≠as]
Esto verifica que el JSON est√© coherente y que COCO lo pueda indexar.

7) Visualizaci√≥n de una imagen con cajas (colores por clase)
Define una paleta de colores por clase (1..6) en BGR para OpenCV:
1: negro
2: rojo
3: azul
4: amarillo
5: cian (azul claro)
6: verde
show_random_image(coco, TRAIN_IMG_DIR):
Toma una imagen aleatoria del COCO.
Recupera sus anotaciones (bboxes y categor√≠as).
Dibuja rect√°ngulos semi-transparentes (con alpha=0.5) del color de la clase (sin texto, solo caja).
Muestra t√≠tulo "<file_name> | <num_boxes> boxes" y la figura.
Te sirve para validar visualmente que las cajas y clases coinciden con lo esperado.

8) Conversi√≥n de val y test a COCO
Finalmente convierte VAL y TEST:
csv_to_coco(VAL_PATH, VAL_IMG_DIR, os.path.join(OUTPUT_DIR,'val_converted.json'))
csv_to_coco(TEST_PATH, TEST_IMG_DIR, os.path.join(OUTPUT_DIR,'test_converted.json'))

<strong>Detalle Genera_tiles.ipynb</strong>

üß± Prop√≥sito general del notebook

Dividir las im√°genes originales del dataset (de gran tama√±o, ~6000√ó4000 px) en fragmentos m√°s peque√±os ‚Äîtiles‚Äî de 1024√ó1024 px con solape (overlap) de 128 px.
Tambi√©n genera nuevas anotaciones COCO para esos tiles, asegurando que cada caja (bounding box) se recorte correctamente y conserve su etiqueta de clase.

üîπ 1. Importaci√≥n de librer√≠as y configuraci√≥n
import os, json, math, cv2
from tqdm import tqdm
from pathlib import Path


os, pathlib ‚Üí manejo de rutas.
json ‚Üí lectura/escritura de anotaciones COCO.
cv2 ‚Üí manipulaci√≥n de im√°genes (OpenCV).
tqdm ‚Üí barra de progreso durante el procesamiento.

üîπ 2. Rutas y par√°metros principales
SRC_IMG = {
  "train": ".../train",
  "val":   ".../val",
  "test":  ".../test",
}
SRC_JSON = {
  "train": ".../train_converted.json",
  "val":   ".../val_converted.json",
  "test":  ".../test_converted.json",
}
OUT_ROOT = ".../tiles"
TILE = 1024        # tama√±o de cada tile
OVERLAP = 128      # solape entre tiles
MIN_VIS_FRAC = 0.3 # fracci√≥n m√≠nima visible para conservar una bbox


üìå Qu√© significa cada uno:
Las rutas apuntan al dataset original (im√°genes y JSONs COCO ya convertidos desde CSV).
TILE = 1024 ‚Üí cada fragmento tendr√° 1024√ó1024 px.
OVERLAP = 128 ‚Üí se solapan parcialmente para no perder objetos en los bordes.
MIN_VIS_FRAC = 0.3 ‚Üí una caja que quede cortada debe conservar al menos un 30 % de su √°rea original para incluirse.

üîπ 3. Funciones auxiliares
load_json(p)
Carga un archivo JSON (anotaciones COCO).
coco_index(coco)
Convierte la estructura COCO a un diccionario indexado por image_id:

{
  image_id: { "file_name":..., "height":..., "width":..., "anns":[...] }
}
para acceder f√°cilmente a las anotaciones de cada imagen.


üîπ 4. Funciones geom√©tricas
intersect_box(box, win)
Devuelve la intersecci√≥n entre la caja box y la ventana (tile) win.
Se usa para recortar las cajas que caen parcialmente dentro de un tile.
area(box)
Calcula el √°rea de una caja [x1, y1, x2, y2].
clamp_to_tile(box, tx, ty)
Ajusta coordenadas al sistema local del tile (restando el offset del tile dentro de la imagen).
to_xyxy(b) y to_xywh(b)
Convierte entre formatos de coordenadas:
[x,y,w,h] ‚Üí [x1,y1,x2,y2]
y viceversa.

üîπ 5. Funci√≥n principal: tile_split(split)
Esta es la parte m√°s importante.
Recorre cada imagen del conjunto split (train, val o test) y crea sus tiles con sus anotaciones.

a) Carga del JSON y prepara salida
coco = load_json(SRC_JSON[split])
idx = coco_index(coco)
out_imgs, out_anns = [], []
out_dir = f"{OUT_ROOT}/{split}_{TILE}_ov{OVERLAP}"
Path(out_dir).mkdir(parents=True, exist_ok=True)

b) Itera sobre cada imagen
Para cada imagen:
Abre el archivo con cv2.imread.
Obtiene h, w.
Recorre la imagen en pasos (step = TILE - OVERLAP), generando ventanas deslizantes.

c) Generaci√≥n de tiles
for ty in range(0, h, step_y):
    for tx in range(0, w, step_x):
        x2, y2 = min(tx + TILE, w), min(ty + TILE, h)


Crea el recorte tile_img = img[ty:y2, tx:x2].
Lo guarda como archivo JPG en la carpeta de salida.


d) Ajuste de las cajas
Para cada anotaci√≥n (bounding box) de la imagen original:
Convierte [x, y, w, h] ‚Üí [x1, y1, x2, y2].
Calcula la intersecci√≥n con el tile.
Si el √°rea visible (vis_frac) ‚â• MIN_VIS_FRAC, se conserva.
Ajusta coordenadas al tile con clamp_to_tile.
Guarda la nueva caja en formato COCO ([x, y, w, h]) con su category_id.
üìå Cada bbox genera una nueva anotaci√≥n con un id √∫nico y el nuevo image_id del tile.


e) Guarda metadatos de cada tile
Si el tile tiene al menos una bbox v√°lida:
out_imgs.append({ "id": new_img_id, "file_name": tile_name, "height": ..., "width": ... })
out_anns.extend(tile_anns_this)
Y suma new_img_id += 1.


üîπ 6. Generaci√≥n del nuevo JSON COCO
Al final de cada split:
out = {
  "info": {"description": f"{split} tiles", "tile": TILE, "overlap": OVERLAP},
  "licenses": [],
  "images": out_imgs,
  "annotations": out_anns,
  "categories": [{"id": i, "name": str(i)} for i in [1,2,3,4,5,6]]
}
with open(f"{OUT_ROOT}/annotations/{split}_tiles.json","w") as f:
    json.dump(out, f, indent=2)

‚úÖ Guarda el archivo train_tiles.json, val_tiles.json, test_tiles.json.


üîπ 7. Bucle final
for sp in ["train","val","test"]:
    tile_split(sp)
Ejecuta el proceso completo para los tres conjuntos.
üì¶ Resultado final
Carpeta /tiles/ con subcarpetas:
tiles/
  ‚îú‚îÄ train_1024_ov128/
  ‚îú‚îÄ val_1024_ov128/
  ‚îú‚îÄ test_1024_ov128/
  ‚îî‚îÄ annotations/
      ‚îú‚îÄ train_tiles.json
      ‚îú‚îÄ val_tiles.json
      ‚îî‚îÄ test_tiles.json
Miles de subim√°genes (.jpg) con tama√±o ~1024√ó1024 px.
JSONs COCO actualizados con las anotaciones recortadas y adaptadas.

<strong>Detalle Train_Faster_R_CNN_sobre_tiles.ipynb</strong>

üß© Prop√≥sito general

Entrenar un modelo Faster R-CNN con ResNet-50 + FPNv2 sobre los tiles generados (subim√°genes 1024√ó1024) y sus anotaciones COCO.
El objetivo: detectar animales en cada tile, con entrenamiento desde pesos preentrenados en COCO.

üîπ 1. Preparaci√≥n del entorno

El notebook empieza instalando y configurando las dependencias:

!pip install albumentations==1.4.8 pycocotools==2.0.7 tqdm opencv-python==4.9.0.80 --quiet
!pip install --upgrade --no-cache-dir torch torchvision torchaudio --quiet


Albumentations ‚Üí para futuras augmentaciones (no se usa directamente aqu√≠, pero est√° lista).

pycocotools ‚Üí para manejar anotaciones COCO.

torch / torchvision ‚Üí red neuronal y modelo de detecci√≥n.

OpenCV ‚Üí lectura y visualizaci√≥n de im√°genes.

tqdm ‚Üí progreso en bucles largos.

Tambi√©n se monta Google Drive si es Colab:

from google.colab import drive
drive.mount('/content/drive')


Y se valida que el entorno detecte CUDA (GPU):

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print("üöÄ Dispositivo:", DEVICE)

üîπ 2. Rutas de dataset y modelo

Define las rutas hacia las im√°genes tileadas y los JSON COCO:

TRAIN_IMG_DIR = "/content/drive/.../tiles/train_1024_ov128"
VAL_IMG_DIR   = "/content/drive/.../tiles/val_1024_ov128"
TEST_IMG_DIR  = "/content/drive/.../tiles/test_1024_ov128"

TRAIN_JSON = "/content/drive/.../tiles/annotations/train_tiles.json"
VAL_JSON   = "/content/drive/.../tiles/annotations/val_tiles.json"
TEST_JSON  = "/content/drive/.../tiles/annotations/test_tiles.json"


Crea tambi√©n la carpeta donde se guardar√°n los modelos:

os.makedirs("/content/drive/.../models", exist_ok=True)

üîπ 3. Definici√≥n del Dataset personalizado
class CocoTiles(CocoDetection)

Hereda de torchvision.datasets.CocoDetection.

Esta clase adapta el dataset COCO para el formato que espera Faster R-CNN:

def __getitem__(self, idx):
    img, target = super().__getitem__(idx)
    img = F.to_tensor(img)

    boxes, labels = [], []
    for ann in target:
        boxes.append(ann["bbox"])
        labels.append(ann["category_id"])

    boxes = torch.as_tensor(boxes, dtype=torch.float32)
    boxes[:, 2:] += boxes[:, :2]  # pasa de [x,y,w,h] a [x1,y1,x2,y2]

    target = {
        "boxes": boxes,
        "labels": torch.as_tensor(labels, dtype=torch.int64),
        "image_id": torch.tensor([idx])
    }
    return img, target


‚úÖ Qu√© hace:

Lee una imagen y sus anotaciones desde COCO.

Convierte a tensores PyTorch.

Ajusta el formato de cajas.

Devuelve el par (imagen_tensor, diccionario_target).

üîπ 4. DataLoaders
train_ds = CocoTiles(TRAIN_IMG_DIR, TRAIN_JSON)
val_ds   = CocoTiles(VAL_IMG_DIR, VAL_JSON)


Se cargan con DataLoader para iterar durante el entrenamiento:

train_dl = DataLoader(train_ds, batch_size=2, shuffle=True,
                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))
val_dl   = DataLoader(val_ds, batch_size=2, shuffle=False,
                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))


El collate_fn es esencial porque cada imagen tiene distinto n√∫mero de cajas; agrupa correctamente los lotes como listas.

üîπ 5. Construcci√≥n del modelo Faster R-CNN
model = fasterrcnn_resnet50_fpn_v2(weights="DEFAULT")


Usa ResNet-50 como backbone y Feature Pyramid Network (FPN) como extractor multiescala.

Preentrenado en COCO, por lo tanto ya entiende caracter√≠sticas visuales generales.

Luego se reemplaza la cabeza de clasificaci√≥n:

in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 7)


üëâ 7 = 6 clases de animales + 1 fondo (background).

Finalmente:

model.to(DEVICE)

üîπ 6. Optimizador y scheduler
opt = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)
sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)


AdamW ‚Üí buen optimizador con regularizaci√≥n incorporada.

CosineAnnealingLR ‚Üí ajusta la tasa de aprendizaje suavemente entre √©pocas.

üîπ 7. Funciones de entrenamiento y evaluaci√≥n
üß† train_one_epoch(epoch)

Ciclo principal de entrenamiento:

def train_one_epoch(epoch):
    model.train()
    total_loss = 0
    for imgs, tgts in tqdm(train_dl, desc=f"Epoch {epoch:02d}"):
        imgs = [im.to(DEVICE) for im in imgs]
        tgts = [{k:v.to(DEVICE) for k,v in t.items()} for t in tgts]
        loss_dict = model(imgs, tgts)
        loss = sum(loss_dict.values())
        opt.zero_grad()
        loss.backward()
        opt.step()
        total_loss += loss.item()
    return total_loss / len(train_dl)


‚úÖ Calcula la p√©rdida de detecci√≥n (clasificaci√≥n + regresi√≥n) y actualiza pesos.

üîé evaluate_val()

Valida el modelo en el conjunto de validaci√≥n con un criterio simple:
compara el n√∫mero de cajas detectadas vs el n√∫mero real (Ground Truth).

@torch.no_grad()
def evaluate_val():
    model.eval()
    mae = []
    for imgs, tgts in val_dl:
        imgs = [im.to(DEVICE) for im in imgs]
        outs = model(imgs)
        for o, t in zip(outs, tgts):
            pred_n = (o["scores"] > 0.5).sum().item()
            true_n = len(t["boxes"])
            mae.append(abs(pred_n - true_n))
    return np.mean(mae)


Devuelve el MAE (Mean Absolute Error) del conteo de detecciones.

üîπ 8. Entrenamiento principal
EPOCHS = 10
for ep in range(1, EPOCHS + 1):
    loss = train_one_epoch(ep)
    mae = evaluate_val()
    sched.step()
    print(f"[Epoch {ep:02d}] loss={loss:.4f} | MAE={mae:.3f}")


Entrena durante 10 √©pocas.

Muestra la p√©rdida media y el error de conteo (MAE).

Durante el entrenamiento se guardan checkpoints cada 2 √©pocas:

torch.save(model.state_dict(), f"/models/checkpoint_epoch_{ep:02d}.pth")


Y al final:

torch.save(model.state_dict(), "/models/fasterrcnn_animals_tiles_final.pth")

üîπ 9. Fine-tuning (entrenamiento adicional)

Despu√©s del modelo base, se ejecuta un nuevo bloque de entrenamiento cargando los pesos previos:

model.load_state_dict(torch.load("/models/fasterrcnn_animals_tiles_final.pth"))


Y se entrena 5 √©pocas m√°s con un learning rate menor, manteniendo el mismo esquema.
Resultado guardado como:

"/models/fasterrcnn_animals_tiles_finetuned.pth"

üîπ 10. Evaluaci√≥n posterior (COCOEval)

El notebook concluye con evaluaci√≥n completa en el conjunto de validaci√≥n:

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval


Genera un JSON con las detecciones del modelo.

Calcula m√©tricas COCO:

AP@[0.5:0.95], AP@0.5, AP@0.75

AR@[1,10,100]

AP/AR por tama√±o (small, medium, large)

Compara modelo base vs fine-tuned.

üîπ 11. Resultados obtenidos (seg√∫n tus logs)
Modelo	AP@[.5:.95]	AP@.5	MAE
Base	0.319	0.557	0.82
Fine-tuned	0.312	0.545	0.87

üëâ El modelo mantuvo desempe√±o similar, con ligera estabilizaci√≥n tras el fine-tuning (menor p√©rdida, pero sin incremento grande en AP global).

üìä En resumen
Etapa	Descripci√≥n
1	Carga dependencias y GPU
2	Define rutas del dataset tileado
3	Implementa clase COCO adaptada
4	Crea DataLoader para entrenamiento y validaci√≥n
5	Carga Faster R-CNN preentrenado
6	Define optimizador y scheduler
7	Entrena modelo durante 10 √©pocas
8	Eval√∫a con MAE (conteo)
9	Fine-tuning (5 √©pocas m√°s)
10	Eval√∫a con m√©tricas COCO (AP, AR)
11	Guarda checkpoints y modelo final
